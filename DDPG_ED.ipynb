{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matlab.engine\n",
    "import socket, struct\n",
    "import threading\n",
    "import concurrent.futures\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## matlab api connection\n",
    "eng = matlab.engine.start_matlab()\n",
    "eng.cd(r'C:\\Users\\pvm8318\\Documents\\NeoVim\\Reinforcement')\n",
    "eng.addpath(r'C:\\Users\\pvm8318\\Documents\\NeoVim\\Reinforcement')\n",
    "def SimRun():\n",
    "    eng.sim('Buck_Converter.slx')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TCP Connection\n",
    "MESSAGE_SIZE = 24\n",
    "DELIMITER = b'\\n'\n",
    "TCP_IP = '156.62.139.28'\n",
    "TCP_PORT = 50000\n",
    "BUFFER_SIZE = MESSAGE_SIZE if MESSAGE_SIZE else 32  # Minimum for two doubles\n",
    "\n",
    "\n",
    "def send_data(conn, val):\n",
    "    \"\"\"Sends two double-precision numbers.\"\"\"\n",
    "    # Fixed Size\n",
    "    msg = struct.pack('>d', val)\n",
    "    conn.send(msg)\n",
    "\n",
    "def receive_data(conn):\n",
    "    \"\"\"Receives three double-precision numbers.\"\"\"\n",
    "    if MESSAGE_SIZE:\n",
    "        data = conn.recv(MESSAGE_SIZE)\n",
    "        val1, val2, Time = struct.unpack('>ddd', data)\n",
    "    else:\n",
    "        # Delimiter\n",
    "        val1 = None\n",
    "        val2 = None\n",
    "        Time = None\n",
    "        while True:\n",
    "            data = conn.recv(BUFFER_SIZE)\n",
    "            if DELIMITER in data:\n",
    "                val1_bytes, remaining = data.split(DELIMITER, 1)\n",
    "                val1 = struct.unpack('>d', val1_bytes)[0]\n",
    "                if DELIMITER in remaining:\n",
    "                    val2_bytes, time_bytes = remaining.split(DELIMITER, 1)\n",
    "                    val2 = struct.unpack('>d', val2_bytes)[0]\n",
    "                    Time = struct.unpack('>d', time_bytes)[0]\n",
    "                    break\n",
    "    return val1, val2, Time\n",
    "\n",
    "# Close the existing socket connection if it is open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Buck converter parameters \n",
    "Vref = 5\n",
    "u = 0\n",
    "R = 1.0  # Resistance\n",
    "L = 0.1  # Inductance\n",
    "C = 1e-3  # Capacitance\n",
    "Vin = 12.0  # Input voltage\n",
    "Vref = 5.0  # Reference output voltage.0\n",
    "# State-space representation of the buck converter\n",
    "A = np.array([[0, 1 / C], [-1 / L, -R / L]])\n",
    "B = np.array([[0], [1 / L]])\n",
    "#steady state calculation\n",
    "duty_cycle =Vref/Vin\n",
    "Iout = Vref/R\n",
    "ILref = Iout/duty_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def websocket ():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((TCP_IP, TCP_PORT))\n",
    "    print('Waiting for Simulink to start')\n",
    "    s.listen(1)\n",
    "    conn, addr = s.accept()\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the actor and critic networks\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = Dense(64, activation='relu')\n",
    "        self.fc2 = Dense(64, activation='relu')\n",
    "        self.output_layer = Dense(action_dim, activation='sigmoid')\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        action = self.output_layer(x) * max_action\n",
    "        return action\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = Dense(64, activation='relu')\n",
    "        self.fc2 = Dense(64, activation='relu')\n",
    "        self.output_layer = Dense(1)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        x = self.fc1(state)\n",
    "        x = tf.concat([x, action], axis=-1)\n",
    "        x = self.fc2(x)\n",
    "        q_value = self.output_layer(x)\n",
    "        return q_value\n",
    "\n",
    "def rewardcal(x, u):\n",
    "    V = x[0]\n",
    "    IL = x[1]\n",
    "    Q = 10*np.eye(2)  # State penalty matrix\n",
    "    R = 1 \n",
    "    reward = -np.linalg.norm(x - np.array([Vref, ILref]))**2 \n",
    "    # reward = -np.linalg.norm(x - np.array([Vref, ILref]))**2 - u**2 * R\n",
    "    return reward\n",
    "\n",
    "\n",
    "def isdone(x, t):\n",
    "    # Define the desirable band\n",
    "    desirable_band = [4.8, 5.2]\n",
    "\n",
    "    # Initialize the start time and t0\n",
    "    t0 = None\n",
    "\n",
    "    V = x[0]\n",
    "    IL = x[1]\n",
    "    \n",
    "    # Check if the state is within the desirable band\n",
    "    if V >= desirable_band[0] and V <= desirable_band[1]:\n",
    "        # Check if t0 is None (first time in the band)\n",
    "        if t0 is None:\n",
    "            t0 = t\n",
    "        # Check if the state has been within the desirable band for 0.5 seconds\n",
    "        elif t - t0 >= 0.5:\n",
    "            return True\n",
    "    else:\n",
    "        # Reset t0 if V gets out of the band\n",
    "        t0 = None\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Initialize environment and hyperparameters\n",
    "state_dim = 2  # I have Voltage and Current that describes the state of the system\n",
    "action_dim = 1  # Duty cycle\n",
    "max_action = 1.0  # Maximum duty cycle value\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.002\n",
    "gamma = 0.99  # Discount factor\n",
    "tau = 0.005  # Target network update rate\n",
    "\n",
    "actor = Actor(state_dim, action_dim, max_action)\n",
    "actor_target = Actor(state_dim, action_dim, max_action)\n",
    "actor_target.set_weights(actor.get_weights())\n",
    "\n",
    "critic = Critic(state_dim, action_dim)\n",
    "critic_target = Critic(state_dim, action_dim)\n",
    "critic_target.set_weights(critic.get_weights())\n",
    "\n",
    "actor_optimizer = Adam(learning_rate=actor_lr)\n",
    "critic_optimizer = Adam(learning_rate=critic_lr)\n",
    "\n",
    "# Define the replay buffer (store experiences)\n",
    "replay_buffer = []\n",
    "num_episodes = 100\n",
    "Vinit = 0\n",
    "Iinit = 0\n",
    "max_steps = 30/1e-5  # Maximum number of steps per episode\n",
    "batch_size = 32  # Replace with your desired batch size\n",
    "\n",
    "# Training loop\n",
    "\n",
    "\n",
    "# After training, use the trained actor network to control the buck converter\n",
    "# You can query the actor network with the current state to get the optimal duty cycle\n",
    "# Remember to adapt this code to your specific Simulink model and requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Simulink to start\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    t1 = threading.Thread(target=SimRun)\n",
    "    t1.start()\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future2 = executor.submit(websocket)\n",
    "        conn = future2.result()\n",
    "    time = 0\n",
    "    state = np.array([Vinit,Iinit])  # Initial state from Simulink\n",
    "    total_reward = 0\n",
    "\n",
    "    while time < 30:\n",
    "        # Choose action using actor network\n",
    "        action = actor(np.expand_dims(state, axis=0))\n",
    "        u = action[0][0]\n",
    "        send_data(conn, u)\n",
    "        val1, val2,time = receive_data(conn)\n",
    "        next_state = np.array([val1, val2])\n",
    "        reward = rewardcal(next_state, u)\n",
    "        done = isdone(next_state, time)\n",
    "        \n",
    "        # Store experience in replay buffer\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "        else:\n",
    "            continue\n",
    "        critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
    "\n",
    "        # Update actor network\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_actions = actor(states)\n",
    "            actor_loss = -tf.reduce_mean(critic(states, new_actions))\n",
    "        actor_gradients = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(zip(actor_gradients, actor.trainable_variables))\n",
    "\n",
    "        # Update target networks\n",
    "        for target, source in zip(actor_target.trainable_variables, actor.trainable_variables):\n",
    "            target.assign(target * (1 - tau) + source * tau)\n",
    "        for target, source in zip(critic_target.trainable_variables, critic.trainable_variables):\n",
    "            target.assign(target * (1 - tau) + source * tau)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward:.2f}\")\n",
    "    print('Duty cycle is:', u)\n",
    "    print('time is:', time)\n",
    "    conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
