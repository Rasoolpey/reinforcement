{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 14:23:27.291396: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-24 14:23:27.293796: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-24 14:23:27.322114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-24 14:23:27.811455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matlab.engine\n",
    "import socket, struct\n",
    "import threading\n",
    "import concurrent.futures\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## matlab api connection\n",
    "eng = matlab.engine.start_matlab()\n",
    "eng.cd(r'/home/pvm8318/Documents/Reinforcement/2023b')\n",
    "eng.addpath(r'/home/pvm8318/Documents/Reinforcement/2023b')\n",
    "def SimRun():\n",
    "    eng.sim('Buck_Converter.slx')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TCP Connection\n",
    "MESSAGE_SIZE = 24\n",
    "DELIMITER = b'\\n'\n",
    "TCP_IP = '127.0.0.1'\n",
    "TCP_PORT = 50000\n",
    "BUFFER_SIZE = MESSAGE_SIZE if MESSAGE_SIZE else 32  # Minimum for two doubles\n",
    "\n",
    "\n",
    "def send_data(conn, val):\n",
    "    \"\"\"Sends two double-precision numbers.\"\"\"\n",
    "    # Fixed Size\n",
    "    msg = struct.pack('>d', val)\n",
    "    conn.send(msg)\n",
    "\n",
    "def receive_data(conn):\n",
    "    \"\"\"Receives three double-precision numbers.\"\"\"\n",
    "    if MESSAGE_SIZE:\n",
    "        data = conn.recv(MESSAGE_SIZE)\n",
    "        val1, val2, Time = struct.unpack('>ddd', data)\n",
    "    else:\n",
    "        # Delimiter\n",
    "        val1 = None\n",
    "        val2 = None\n",
    "        Time = None\n",
    "        while True:\n",
    "            data = conn.recv(BUFFER_SIZE)\n",
    "            if DELIMITER in data:\n",
    "                val1_bytes, remaining = data.split(DELIMITER, 1)\n",
    "                val1 = struct.unpack('>d', val1_bytes)[0]\n",
    "                if DELIMITER in remaining:\n",
    "                    val2_bytes, time_bytes = remaining.split(DELIMITER, 1)\n",
    "                    val2 = struct.unpack('>d', val2_bytes)[0]\n",
    "                    Time = struct.unpack('>d', time_bytes)[0]\n",
    "                    break\n",
    "    return val1, val2, Time\n",
    "\n",
    "# Close the existing socket connection if it is open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Buck converter parameters \n",
    "Vref = 5\n",
    "u = 0\n",
    "R = 1.0  # Resistance\n",
    "L = 0.1  # Inductance\n",
    "C = 1e-3  # Capacitance\n",
    "Vin = 12.0  # Input voltage\n",
    "Vref = 5.0  # Reference output voltage.0\n",
    "# State-space representation of the buck converter\n",
    "A = np.array([[0, 1 / C], [-1 / L, -R / L]])\n",
    "B = np.array([[0], [1 / L]])\n",
    "#steady state calculation\n",
    "duty_cycle =Vref/Vin\n",
    "Iout = Vref/R\n",
    "ILref = Iout/duty_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def websocket ():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((TCP_IP, TCP_PORT))\n",
    "    print('Waiting for Simulink to start')\n",
    "    s.listen(1)\n",
    "    conn, addr = s.accept()\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the actor and critic networks\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = Dense(64, activation='relu')\n",
    "        self.fc2 = Dense(64, activation='relu')\n",
    "        self.output_layer = Dense(action_dim, activation='sigmoid')\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        action = self.output_layer(x) * max_action\n",
    "        return action\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = Dense(64, activation='relu')\n",
    "        self.fc2 = Dense(64, activation='relu')\n",
    "        self.output_layer = Dense(1)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        x = self.fc1(state)\n",
    "        action = tf.squeeze(action, axis=[1,2])  # remove dimensions of size 1\n",
    "        x = tf.concat([x, action], axis=-1)\n",
    "        x = self.fc2(x)\n",
    "        q_value = self.output_layer(x)\n",
    "\n",
    "def rewardcal(x, u):\n",
    "    V = x[0]\n",
    "    IL = x[1]\n",
    "    Q = 10*np.eye(2)  # State penalty matrix\n",
    "    R = 1 \n",
    "    reward = -np.linalg.norm(x - np.array([Vref, ILref]))**2 \n",
    "    # reward = -np.linalg.norm(x - np.array([Vref, ILref]))**2 - u**2 * R\n",
    "    return reward\n",
    "\n",
    "\n",
    "def isdone(x, t):\n",
    "    # Define the desirable band\n",
    "    desirable_band = [4.8, 5.2]\n",
    "\n",
    "    # Initialize the start time and t0\n",
    "    t0 = None\n",
    "\n",
    "    V = x[0]\n",
    "    IL = x[1]\n",
    "    \n",
    "    # Check if the state is within the desirable band\n",
    "    if V >= desirable_band[0] and V <= desirable_band[1]:\n",
    "        # Check if t0 is None (first time in the band)\n",
    "        if t0 is None:\n",
    "            t0 = t\n",
    "        # Check if the state has been within the desirable band for 0.5 seconds\n",
    "        elif t - t0 >= 0.5:\n",
    "            return True\n",
    "    else:\n",
    "        # Reset t0 if V gets out of the band\n",
    "        t0 = None\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Initialize environment and hyperparameters\n",
    "state_dim = 2  # I have Voltage and Current that describes the state of the system\n",
    "action_dim = 1  # Duty cycle\n",
    "max_action = 1.0  # Maximum duty cycle value\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.002\n",
    "gamma = 0.99  # Discount factor\n",
    "tau = 0.005  # Target network update rate\n",
    "\n",
    "actor = Actor(state_dim, action_dim, max_action)\n",
    "actor_target = Actor(state_dim, action_dim, max_action)\n",
    "actor_target.set_weights(actor.get_weights())\n",
    "\n",
    "critic = Critic(state_dim, action_dim)\n",
    "critic_target = Critic(state_dim, action_dim)\n",
    "critic_target.set_weights(critic.get_weights())\n",
    "\n",
    "actor_optimizer = Adam(learning_rate=actor_lr)\n",
    "critic_optimizer = Adam(learning_rate=critic_lr)\n",
    "\n",
    "# Define the replay buffer (store experiences)\n",
    "replay_buffer = []\n",
    "num_episodes = 100\n",
    "Vinit = 0\n",
    "Iinit = 0\n",
    "max_steps = 30/1e-5  # Maximum number of steps per episode\n",
    "batch_size = 32  # Replace with your desired batch size\n",
    "\n",
    "# Training loop\n",
    "\n",
    "\n",
    "# After training, use the trained actor network to control the buck converter\n",
    "# You can query the actor network with the current state to get the optimal duty cycle\n",
    "# Remember to adapt this code to your specific Simulink model and requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    t1 = threading.Thread(target=SimRun)\n",
    "    t1.start()\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future2 = executor.submit(websocket)\n",
    "        conn = future2.result()\n",
    "    time = 0\n",
    "    state = np.array([Vinit,Iinit])  # Initial state from Simulink\n",
    "    total_reward = 0\n",
    "\n",
    "    while time < 30:\n",
    "        # Choose action using actor network\n",
    "        u = action[0][0]\n",
    "        send_data(conn, u)\n",
    "        val1, val2,time = receive_data(conn)\n",
    "        next_state = np.array([val1, val2])\n",
    "        reward = rewardcal(next_state, u)\n",
    "        done = isdone(next_state, time)\n",
    "        action = actor(np.expand_dims(state, axis=0))\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Update critic network\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            continue\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        states = np.array([x[0] for x in batch])\n",
    "        actions = np.array([x[1] for x in batch])\n",
    "        rewards = np.array([x[2] for x in batch])\n",
    "        next_states = np.array([x[3] for x in batch])\n",
    "        dones = np.array([x[4] for x in batch])\n",
    "        next_actions = actor_target(next_states)\n",
    "        target_q_values = critic_target(next_states, next_actions)\n",
    "        target_q_values = rewards + gamma * target_q_values * (1 - dones)\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = critic(states, actions)\n",
    "            critic_loss = tf.reduce_mean(tf.square(target_q_values - q_values))\n",
    "        critic_gradients = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "        # Store experience in replay buffer\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "        else:\n",
    "            continue\n",
    "        critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
    "\n",
    "        # Update actor network\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_actions = actor(states)\n",
    "            actor_loss = -tf.reduce_mean(critic(states, new_actions))\n",
    "        actor_gradients = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(zip(actor_gradients, actor.trainable_variables))\n",
    "\n",
    "        # Update target networks\n",
    "        for target, source in zip(actor_target.trainable_variables, actor.trainable_variables):\n",
    "            target.assign(target * (1 - tau) + source * tau)\n",
    "        for target, source in zip(critic_target.trainable_variables, critic.trainable_variables):\n",
    "            target.assign(target * (1 - tau) + source * tau)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward:.2f}\")\n",
    "        print('Duty cycle is:', u)\n",
    "        print('time is:', time)\n",
    "        if done:\n",
    "            break\n",
    "    conn.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Simulink to start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 13:14:40.890900: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: ConcatOp : Ranks of all input tensors should match: shape[0] = [2] vs. shape[1] = [1,1,1]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling Critic.call().\n\n\u001b[1m{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [2] vs. shape[1] = [1,1,1] [Op:ConcatV2] name: concat\u001b[0m\n\nArguments received by Critic.call():\n  • state=tf.Tensor(shape=(2,), dtype=float32)\n  • action=tf.Tensor(shape=(1, 1, 1), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 87\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Update Critic network\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 87\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Reshape action tensor\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     critic_loss \u001b[38;5;241m=\u001b[39m loss_function(target_q_values, q_values)\n\u001b[1;32m     89\u001b[0m critic_gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(critic_loss, critic\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[35], line 28\u001b[0m, in \u001b[0;36mCritic.call\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[0;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[1;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling Critic.call().\n\n\u001b[1m{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [2] vs. shape[1] = [1,1,1] [Op:ConcatV2] name: concat\u001b[0m\n\nArguments received by Critic.call():\n  • state=tf.Tensor(shape=(2,), dtype=float32)\n  • action=tf.Tensor(shape=(1, 1, 1), dtype=float32)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MATLAB System block 'Buck_Converter/TCP//IP Receive' error occurred when invoking 'stepImpl' method of 'instrument.system.TCPIPReceive'. The error was thrown from '\n",
      " <a href=\"matlab:opentoline('/usr/local/MATLAB/R2023b/toolbox/instrument/instrumentblks/+instrument/+system/TCPIPReceive.m',388, 0)\">'/usr/local/MATLAB/R2023b/toolbox/instrument/instrumentblks/+instrument/+system/TCPIPReceive.m'</a> at line 388'.\n",
      "Caused by:\n",
      "    Error receiving data from the remote server.\n",
      "    Additional Information: Operation timed out before requested data was received.\n",
      "\n",
      "Exception in thread Thread-26 (SimRun):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/pvm8318/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_318872/1695220910.py\", line 6, in SimRun\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/matlab/engine/matlabengine.py\", line 71, in __call__\n",
      "    _stderr, feval=True).result()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/matlab/engine/futureresult.py\", line 67, in result\n",
      "    return self.__future.result(timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/matlab/engine/fevalfuture.py\", line 82, in result\n",
      "    self._result = pythonengine.getFEvalResult(self._future,self._nargout, None, out=self._out, err=self._err)\n",
      "matlab.engine.MatlabExecutionError: MATLAB System block 'Buck_Converter/TCP//IP Receive' error occurred when invoking 'stepImpl' method of 'instrument.system.TCPIPReceive'. The error was thrown from '\n",
      " <a href=\"matlab:opentoline('/usr/local/MATLAB/R2023b/toolbox/instrument/instrumentblks/+instrument/+system/TCPIPReceive.m',388, 0)\">'/usr/local/MATLAB/R2023b/toolbox/instrument/instrumentblks/+instrument/+system/TCPIPReceive.m'</a> at line 388'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# Define the shared network architecture\n",
    "def shared_network(state_dim, action_dim):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(state_dim,))\n",
    "\n",
    "    # Shared hidden layers\n",
    "    hidden1 = Dense(64, activation='relu')(inputs)\n",
    "    hidden2 = Dense(64, activation='relu')(hidden1)\n",
    "\n",
    "    # Actor head (output for action)\n",
    "    actor_output = Dense(action_dim, activation='sigmoid')(hidden2)  # Output range: [0, 1]\n",
    "\n",
    "    # Critic head (output for state value)\n",
    "    critic_output = Dense(1)(hidden2)  # Linear activation for value estimation\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[actor_output, critic_output])\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "state_dim = 2  # Number of state features (Vc and IL)\n",
    "action_dim = 1  # Single action (duty cycle)\n",
    "\n",
    "# Create the shared network\n",
    "shared_net = shared_network(state_dim, action_dim)\n",
    "\n",
    "# Print the model summary\n",
    "shared_net.summary()\n",
    "\n",
    "\n",
    "# Initialize Actor and Critic networks\n",
    "actor = Actor(state_dim, action_dim, max_action)\n",
    "critic = Critic(state_dim, action_dim)\n",
    "\n",
    "# Initialize target networks\n",
    "target_actor = Actor(state_dim, action_dim, max_action)\n",
    "target_critic = Critic(state_dim, action_dim)\n",
    "\n",
    "# Copy weights from original networks to target networks\n",
    "target_actor.set_weights(actor.get_weights())\n",
    "target_critic.set_weights(critic.get_weights())\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Define loss function\n",
    "loss_function = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define replay buffer\n",
    "replay_buffer = []\n",
    "\n",
    "# Define noise process\n",
    "noise_process = np.random.normal(0, 0.1, size=(action_dim,))\n",
    "\n",
    "# Define training loop\n",
    "for episode in range(1000):  # number of episodes\n",
    "    # Reset the environment and get the initial state\n",
    "    t1 = threading.Thread(target=SimRun)\n",
    "    t1.start()\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future2 = executor.submit(websocket)\n",
    "        conn = future2.result()\n",
    "    time = 0\n",
    "    state = np.array([Vinit,Iinit])  # Initial state from Simulink\n",
    "    total_reward = 0\n",
    "    u=0\n",
    "    while time<30:  # maximum steps per episode\n",
    "        # Select action\n",
    "        send_data(conn, u)\n",
    "        val1, val2,time = receive_data(conn)\n",
    "        next_state = np.array([val1, val2])\n",
    "        reward = rewardcal(next_state, u)\n",
    "        done = isdone(next_state, time)\n",
    "        action = actor(np.expand_dims(state, axis=0))\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Sample a random minibatch of N transitions from the replay buffer\n",
    "        if len(replay_buffer) >= 64:\n",
    "            batch = random.sample(replay_buffer, 64)\n",
    "        else:\n",
    "            batch = []\n",
    "\n",
    "        # Update Critic network\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = critic(state, tf.expand_dims(action, axis=0))  # Reshape action tensor\n",
    "            critic_loss = loss_function(target_q_values, q_values)\n",
    "        critic_gradients = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
    "\n",
    "        # Update Actor network\n",
    "        with tf.GradientTape() as tapeimport tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = Dense(64, activation='relu')\n",
    "        self.fc2 = Dense(64, activation='relu')\n",
    "        self.output_layer = Dense(action_dim, activation='tanh')\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return self.max_action * x\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = Dense(64, activation='relu')\n",
    "        self.fc2 = Dense(64, activation='relu')\n",
    "        self.output_layer = Dense(1)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        x = tf.concat([state, action], axis=-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.output_layer(x)\n",
    "actor_gradients, actor.trainable_variables))\n",
    "\n",
    "        # Update target networks\n",
    "        target_actor.set_weights(target_actor.get_weights() * 0.995 + actor.get_weights() * 0.005)\n",
    "        target_critic.set_weights(target_critic.get_weights() * 0.995 + critic.get_weights() * 0.005)\n",
    "\n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward:.2f}\")\n",
    "        print('Duty cycle is:', u)\n",
    "        print('time is:', time)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Simulink to start\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rewardcal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m val1, val2,time \u001b[38;5;241m=\u001b[39m receive_data(conn)\n\u001b[1;32m     62\u001b[0m next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([val1, val2])\n\u001b[0;32m---> 63\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[43mrewardcal\u001b[49m(next_state, u)\n\u001b[1;32m     64\u001b[0m done \u001b[38;5;241m=\u001b[39m isdone(next_state, time)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Compute the advantage for this state-action pair\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rewardcal' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MATLAB System block 'Buck_Converter/TCP//IP Receive' error occurred when invoking 'stepImpl' method of 'instrument.system.TCPIPReceive'. The error was thrown from '\n",
      " <a href=\"matlab:opentoline('/usr/local/MATLAB/R2023b/toolbox/instrument/instrumentblks/+instrument/+system/TCPIPReceive.m',388, 0)\">'/usr/local/MATLAB/R2023b/toolbox/instrument/instrumentblks/+instrument/+system/TCPIPReceive.m'</a> at line 388'.\n",
      "Caused by:\n",
      "    Error receiving data from the remote server.\n",
      "    Additional Information: Operation timed out before requested data was received.\n",
      "\n",
      "Exception in thread Thread-6 (SimRun):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/pvm8318/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_436457/1695220910.py\", line 6, in SimRun\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/matlab/engine/matlabengine.py\", line 71, in __call__\n",
      "    _stderr, feval=True).result()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/matlab/engine/futureresult.py\", line 67, in result\n",
      "    return self.__future.result(timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/matlab/engine/fevalfuture.py\", line 82, in result\n",
      "    self._result = pythonengine.getFEvalResult(self._future,self._nargout, None, out=self._out, err=self._err)\n",
      "matlab.engine.MatlabExecutionError: MATLAB System block 'Buck_Converter/TCP//IP Receive' error occurred when invoking 'stepImpl' method of 'instrument.system.TCPIPReceive'. The error was thrown from '\n",
      " <a href=\"matlab:opentoline('/usr/local/MATLAB/R2023b/toolbox/instrument/instrumentblks/+instrument/+system/TCPIPReceive.m',388, 0)\">'/usr/local/MATLAB/R2023b/toolbox/instrument/instrumentblks/+instrument/+system/TCPIPReceive.m'</a> at line 388'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import numpy as np\n",
    "\n",
    "# Define the shared network architecture\n",
    "def shared_network(state_dim, action_dim):\n",
    "    inputs = Input(shape=(state_dim,))\n",
    "    hidden1 = Dense(64, activation='relu')(inputs)\n",
    "    hidden2 = Dense(64, activation='relu')(hidden1)\n",
    "    actor_output = Dense(action_dim, activation='sigmoid')(hidden2)  # Output range: [0, 1]\n",
    "    critic_output = Dense(1)(hidden2)  # Linear activation for value estimation\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[actor_output, critic_output])\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "state_dim = 2  # Number of state features (Vc and IL)\n",
    "action_dim = 1  # Single action (duty cycle)\n",
    "Vinit = 0\n",
    "Iinit = 0\n",
    "max_steps = 30/1e-5 \n",
    "# Create the shared network\n",
    "shared_net = shared_network(state_dim, action_dim)\n",
    "\n",
    "# Define optimizer and loss functions\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Actor loss function (policy gradient with advantage)\n",
    "def actor_loss(log_probs, advantages):\n",
    "    return -tf.reduce_mean(log_probs * advantages)\n",
    "\n",
    "# Critic loss function (mean squared error)\n",
    "def critic_loss(targets, predicted_values):\n",
    "    return tf.reduce_mean(tf.square(targets - predicted_values))\n",
    "\n",
    "# Compute advantages for a specific state-action pair\n",
    "def compute_advantage(q_value, v_value):\n",
    "    return q_value - v_value\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 1000  # Number of episodes\n",
    "runtime = 30  # Maximum steps per episode\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    t1 = threading.Thread(target=SimRun)\n",
    "    t1.start()\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future2 = executor.submit(websocket)\n",
    "        conn = future2.result()\n",
    "    # Reset the environment and get the initial state\n",
    "    state = np.array([Vinit, Iinit])  # Replace with actual initial state\n",
    "    total_reward = 0\n",
    "    time = 0\n",
    "    u=0\n",
    "    while time < runtime:\n",
    "        \n",
    "        action = shared_net(np.expand_dims(state, axis=0))[0][0] \n",
    "        u=action\n",
    "        send_data(conn, u)\n",
    "        val1, val2,time = receive_data(conn)\n",
    "        next_state = np.array([val1, val2])\n",
    "        reward = rewardcal(next_state, u)\n",
    "        done = isdone(next_state, time)\n",
    "\n",
    "        # Compute the advantage for this state-action pair\n",
    "        q_value = shared_net(np.expand_dims(state, axis=0))[1][0]\n",
    "        v_value = shared_net(np.expand_dims(state, axis=0))[1][0]\n",
    "        advantage = compute_advantage(q_value, v_value)\n",
    "\n",
    "        # Update the Actor network\n",
    "        with tf.GradientTape() as tape:\n",
    "            log_prob = tf.math.log(action)  # Assuming continuous action space\n",
    "            actor_loss_value = actor_loss(log_prob, advantage)\n",
    "        actor_gradients = tape.gradient(actor_loss_value, shared_net.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(zip(actor_gradients, shared_net.trainable_variables))\n",
    "\n",
    "        # Update the Critic network\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_value = total_reward  # Compute the return for the trajectory\n",
    "            critic_loss_value = critic_loss(target_value, shared_net(np.expand_dims(state, axis=0))[1][0])\n",
    "        critic_gradients = tape.gradient(critic_loss_value, shared_net.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(zip(critic_gradients, shared_net.trainable_variables))\n",
    "\n",
    "        # Update state for the next step\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        # Print the total reward for the episode\n",
    "        print(\"reward is:\", reward)\n",
    "        print('Duty cycle is:', u)\n",
    "        print('time is:', time)\n",
    "\n",
    "        if done:\n",
    "            break  # End the episode if termination condition is met\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
